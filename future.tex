\chapter{Future Work}\label{C:future}

From here there are several different approaches that David and I propose. The first consideration is to make it more difficult for a test to be similar to another. One method is to hold more data about the method calls. The extra data that seems to be the next stage is parameters. This involves holding the primitive values. To hold every occurrence of the data would be unnecessary as it would be good enough to hold which side the value is on. For example, true and false would be held, and for number values, the values that would be held are -1, 0 and 1. This gives us more information such as when an edge case is being tested such as a negative value.

Since this project is to not say whether two tests are the same, but to suggest to a user that two tests may be the same and they require human observation. It would be interesting to see statistics about the results. One method of providing this is through graph components. 

\section{Evaluation}
The method of evaluation is going to be through how well the framework allows for redundant tests to be identified. To do this manual inspection of the tests will have to be done in order to judge how well the framework is identifying potential redundant tests. Using actively developed benchmarks allows for this information to be relayed to these developers and ask for their thoughts on the tests.

\section{Final Report}
\todo{todo}

\section{Requests for feedback}
\todo{todo}