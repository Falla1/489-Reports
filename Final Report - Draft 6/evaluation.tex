\chapter{Evaluation}\label{C:evaluation}

\section{Pipeline Length Comparison}
\label{sec:pipelineEva}
An interesting result was the impact of the length of the pipeline had on the total time taken to analyse the data. The idea of the pipeline was to decrease the amount of comparisons that the each stage in the pipeline has to do. This in turn meant less memory consumption and time taken for the more computationally heavy stages. Looking at the results show that in most cases, having two pipeline stages meant a significantly decreased total time taken to analyse over a pipeline with three stages. Intuitively, this may go against what would make sense. Looking at Figure \ref{fig:pipelinegraph}, it shows that there is a major decrease in the number of comparisons that the last stage does between one stage and two or three stages. However, between two and three stages there is limited differences. These results imply that the test cases that took less time using a two stage, were spending more time on the second stage than they were saving from the reduced number of comparisons during the third stage. 

There would be two different reasons that this result may occur. Firstly, each benchmark had it's own level of redundancy that was being looked for, this was dependent on the size of the test cases of each. As benchmarks with smaller test cases needed to have a higher level of redundancy in comparison to benchmarks with larger test cases, as this meant they would have roughly the same percentage of difference. Since each benchmark had their own percentage, this implies that each benchmark also has its optimal settings for the second pipeline. The settings were chosen by experimenting with different options. Therefore, not using a near optimal second pipeline could have contributed to it. \todo{Need to explain it better}

The second reason is that there may be little difference between the second pipeline and third pipeline. For example, if the first pipeline reduces the number of comparisons needed to 50, then there is a limited number of comparisons that a second pipeline can remove. This leads to a situation where the number of identified redundant test cases is similar for both pipelines.

As expected of the redundant tests identified, for each benchmark there was no significant change. If there were a change between the two pipelines with the same final stage settings could imply two things. Firstly, there is a bug in the code. Secondly, the second pipeline stage is more specific than the last. Since both the second stage and third stage share the same final stage, this would cause the last stage to be redundant and the redundant tests different to that of the two stage pipeline. \todo{@DJ, Does this make sense?}

\section{K Length Comparison}

Increasing the depth of the calling context means adding data to the analysis stages, intuitively making it more difficult for test cases to be the same. This should mean there is a decrease in the number of comparisons as the calling context depth increases. It also means that more data is going to have to be looked at which implies an increase in time as calling context depth increases.

Examining Figure \ref{fig:kdepthgraph}, it shows that the number of redundant tests is only slightly decreased as the K depth increases. This may show several things. Firstly, it may imply that when no other settings such as weighting or parameters are used, then the depth has limited effect on the redundant test cases identified. This can be seen how there is a limited amount of change for each of the benchmarks as the K increases. Secondly, the number of comparisons that were output from the first pipeline stage may cause there to be a limited differentiation. If the first pipeline stage output a low number of comparisons, then the comparisons output would more likely to be similar regardless of the K depth specified. This is due to the amount of information that the analysis stage has access to. If two test cases are the practically the same method calls, except for the parameters passed to methods, then the K depth will not pick this difference up.

Overall the results imply that for some projects the calling context does not have to be particularly deep when there are no other settings used. Section \ref{sec:param} and Section \ref{sec:weight} explore the impact that using parameters has on the output.

\todo{Maybe look at a coding for the different types of tests identified. What types does lower K mean more likely to pick up ?}

\section{Parameter Comparison}
\label{sec:param}
\todo{Appears that parameters may increase the non-deterministic attirbutes of java as mentioned.}
Intuitively, taking into account for parameters of method calls would increase the time taken to calculate the level of redundancy due to the extra amount of data. However, parameters only add extra computation when the method execution of the given K is exactly the same. For example, if a test execution is A - \textgreater B - \textgreater C with D Parameter is compared to A - \textgreater B - \textgreater F with E Parameter. Then the parameter won't be taken into account and therefore add limited amount of time. Another method which would cause time to be added is the amount of time that the VM spends garbage collecting, due to increased amount of information in memory, the garbage collection process will have to free up more memory for the analysis to continue to run.

A larger calling context would mean that it would be more difficult for two tests to match. Matching with a K value of three would be expected to be a rare occurrence in the scale of a benchmark. The parameters can be large and hold a lot of information therefore if the K value matches, then matching parameters may incur a large cost and would be expected to have limited matching. Taking into account this extra information, it would make sense that the number of redundant tests identified would decrease, but the total time taken would increase. 

Looking at the significant Table \ref{fig:paramgraph}, this matches what is expected to occur. For every benchmark, using parameters significantly increases the time taken. This may imply that the number of tests that match the K value of three was higher than expected, causing more computations to look at the parameter information. Examining Appendix \ref{fig:paramtime} shows how some benchmarks react differently to benchmarks. The most interesting would be how Jasm reacts to the use of parameters. Without parameters, it analyses the data in less than one minute, with parameters, it takes just under five minutes. One reason this may occur is the number of tests that match K value is higher than average, without weighting the set up methods are taken into account. If these set up methods are a large majority of the method calls, and match each other for the K value and parameters, this would increase the time taken substantially. Examining the weighting and parameter time taken in Appendix \ref{fig:weightparamtime}, it shows that when weighting is taken into account with parameters then for Jasm, the time taken goes from just under five minutes to just under one minute.

Every benchmark caused a significant decrease in the number of redundant test cases. This backs up the implication that there were more situations where a K depth of three matched than expected meaning that parameters had a significant effect on the redundant tests identified. Looking at Figure \ref{fig:paramgraph} shows that every benchmark reacted with a substantial decrease in redundant tests identified, with Whiley being the least affected. 

\todo{Test on Calling context = 2 ? Look at significant differences}

\section{Weighting Comparison}
\label{sec:weight}
\todo{Weighting appears to reduce the variance in the time taken}
As previously discussed in \todo{section 2 and 3} many of the other papers encountered difficulties when test cases shared setup and teardown method calls, meaning that some of the methods picked up caused a high false - positive rate. Intuitively this makes sense when the test cases are small or the setup and teardown is large, meaning the setup and teardown methods make up a larger majority of the test calls. By taking the approach of removing a portion of the most executed method calls meant that the amount of data that would be compared decreases. This should reflect onto the results by showing a decrease in the time taken. The effect that it has on the number of redundant test cases should be dependent on the benchmark when weighting is used. This is because removing the most common tests should imply an decreased false-positive rate, but also may increase the number of actual redundant tests picked up. Therefore, depending on which on the two variables outweighs the other, will result in an increase or decrease in the number of redundant test cases identified.

Looking at the results in Table \ref{weightingsig}, it shows there was a mixture of results in regard to the total time taken. Two out of the three benchmarks that significantly increased in total time were from the small benchmarks. This may imply that the size of the benchmark has some relation to the effect of weighting on the time taken. The reason is, weighting is calculated once for each test case at the start of the pipeline stage. This weighting calculation can take a varying length of time, depending on the number of test cases. Since the weighting is linear in relation to the number of test cases, however, the comparisons is factorial in relation to the test cases. This shows that calculating the weighting for smaller benchmarks may take more time than it saves. However, the goal of weighting is not to save time, but it is to reduce the number of false positives.

Spring was the only benchmark that significantly increased the number of redundant test cases identified. The other benchmarks had a significant decrease. At first, this makes it appear that by using weighting it may solve some of the issues that were identified in \cite{koochakzadeh2009test} \cite{li2008static}. To confirm this, examining Table \ref{whileycoding} and \ref{metriccoding} will give insight into the effect of weighting. Comparing the two columns, Pipeline 2 and Weighting it shows the only difference is a removal of the two limited redundancy test cases that were picked up by Pipeline 2. The weighting is able to remove the test cases that have similar set up and tear down methods, but fails to reduce the number of other redundancies. By removing the method executions that were common throughout the benchmarks, this lead to a decrease in false-positive tests identified.


\section{Weighting and Parameter Comparison}
\todo{More}
The result from a combination of both, weighting and parameter provides an interesting set of results. Looking at Table \ref{weightingparamsig}, it shows that it is nearly identical to the parameters table with the only exception being that the Imcache benchmark changed from significant increase in time to a significant decrease in time. This indicates that parameters may have a stronger effect on the final outcome in comparison to weighting. This is an unexpected result. As previously discussed in Section \ref{sec:param}, one of the reasons that parameters increase the time taken is due to the similar setup and tear down method calls, causing the number of k depth matches to increase and the parameters to be examined more often. With weighting removing a large portion of the common method executions it would be expected that the portion of K depth matches decreases in relation to the number of method calls, causing a decrease in time between weighting and parameters combined and parameters. Looking at Figure \ref{fig:weightparamvparamtime}, this appears to be true for Jasm, Metric-x, Spring and Imcache. However, for Whiley and Ant benchmarks the time to calculate the weighting information is taking longer than the comparisons saved. 

\section{Contributions}

\subsection{Can method execution details give a good overview of the level of test redundancy?}

It is difficult to make a direct comparison between using statement information and higher level method execution details without implementing and comparing both on the same benchmarks. Therefore, the level of redundancy identified will be evaluated by the potential redundancy identified as well as the type of redundancies. It is important to take into consideration both. Knowing the number of tests allows us to see how method executions handle different types of tests, unit vs end to end respectively. The potential redundancy gives us insight into the types of redundant tests identified, giving information on which types of test redundancy is easier to pick up and which is harder when using method execution.

The use of method execution allows us to look at test cases that produce a larger amount of total information, in particular ones which are end to end tests (Whiley) which produce a large amount of data. It would be interesting to see the comparison between statement information and method execution details. It appears to be intuitive that statement coverage would give more accuracy in regard to determining whether two test cases are redundant but this would come at a cost of time and memory consumption. What is the optimal trade-off for this situation? \todo{Maybe talk about it here?}

By using a higher abstraction of information, there appears to be no relation to the time taken and the number of redundant test cases with the two types of test cases, unit and end to end respectively.

Examining the Table \ref{whileycoding} and \ref{metriccoding}, allows for an comparison to identify the difference between end to end tests and unit tests while using method execution details. It appears that the types of the tests may react differently to the settings used. For the end to end test of Whiley, where the size of a single test case is larger than Metric-x, it appears to respond to the use of parameters and weighting in a positive manner. Where the tests that are picked up as limited redundancy by the Pipeline 2 are both removed when weighting and parameters are used. In comparison to the unit test case. The unit test appeared to not respond either to weighting or parameters by identifying tests that had limited redundancy. When combining both of these, it removed the tests that were identified, but had limited redundancy. \todo{Explain why it might be happening} This may reflect on the amount of pre processing and information needed for the different types to get identified. \todo{This would mean that the larger bench marks can be analysed faster ?}


\subsection{Can we improve the overall performance with weighting?}
\todo{More}
Weighting attempts to improve the accuracy of the redundancy by removing false-positives while improving the time taken to analyse. 

\todo{Examine the coding tables, state that using weighting can help}

\subsection{Can we improve the performance using parameters?}

The answer to the question depends on the type of parameters that are used. Initially the only parameters that were being retrieved were the primitive types. The reason was due to the easy nature of retrieving the values of them. This lead to a less than expected outcome as the majority of parameters involved object types which contained the important information that could be used to increase the separability of tests. By using reflection to retrieve the fields of these objects allowed more insight into the content that the parameter objects were holding and the nature of them, but it also meant more data had to be held and analysed resulting in an increased time taken. There exists this trade off between time taken and confidence of redundancy when using parameters. The reason for running a framework such as this is to reduce the number of redundant test cases, therefore the majority of situations will prefer a higher confidence of redundancy over time taken. The answer is then to use reflection to retrieve the parameters.

\todo{Examine the coding tables, state that using parameters improves performance}

\subsection{Can Calling Trees we used to better identify redundant tests?}

The result was unexpected. It showed that as the depth of the calling context increased, there was limited level of reduction in the number of redundant tests identified. Although there was a change, it was lower that what was expected. A possible reason for this was after a certain number of calling context, the tests that were going to be different, were already different and increasing the size of the calling tree did not affect it. This may imply that using a calling context depth of two is enough to generate a list of redundant tests similar to a depth of two. \todo{Todo check with K Length Comparison - might be talking about same things}

Using a calling tree allows for the analysing tool to better identify redundant tests as shown by the comparison above where the one, two and three calling sizes are compared.

\subsection{How does pipelining impact the time taken?}

Pipelining was one of the critical aspects of the project. It not only lead to a decrease in the time taken but also the memory consumed. Like calling trees, there is an optimal number of pipelines before they are causing a larger increase in time than they save. We can see in the results that each \todo{check results} benchmark responded differently to the length of the pipeline. This may have been due to not finding the optimal pipeline variables for the second pipeline causing a large number of comparisons still being needed to be done by the last pipeline. Another reasons may have been due to the nature of the tests where they were difficult to separate until you used more data such as a list or calling context. 

\todo{Maybe guidelines for the settings that each type of test should use?}

\section{Justify your approach}

\todo{Unsure if I should put anything ehre}


\section{Critically evaluate your study}
\label{sec:crit}
A major limitation to the study was volatile memory. For test suites such as Whiley, storing the parameter data and calling context endured a high level of volatile memory usage, limiting the amount of tests for Whiley that were able to be retrieved (Limited to valid tests). An approach that should have been considered with more thought was the use of a database. Although this would have introduced difficulty reducing the non-deterministic behaviour to a minimum. A major factor would be the difficulty around executing the analysis on a distributed system and replicating the conditions each run.

Some of the benchmarks that were chosen contain multiple unit tests within one test case. The framework described throughout this paper would not be able to pick up when a test case subsumes another. The frame work only looks at the total method set, and for each of the singular unit tests, there would need to be a skewed proportion of the conjoined test case in regard to the total method executions. This means that unless one of the unit tests within the conjoined test case was arbitrary larger than the other, roughly 98:1 lines of code then it would not be picked up as being redundant, unless there was another conjoined test case similar. This is where the other approaches identified in Chapter \ref{C:related} would be more effective. By looking at the statement information, it is easier to determine when a test is a subset of another and have more confidence on it being a redundant test case in comparison to looking at solely the method execution details. \todo{Need to explain this better.}

As discussed in Section \ref{sec:pipelineEva}, some benchmarks performed better with a two stage pipeline in comparison to a three stage, and vice versa. This may have been due to not being able to identify the optimal parameters for each benchmark, although there were multiple different parameters tested to explore each benchmark's optimal. \todo{Maybe explore how I decided the optimal ?}